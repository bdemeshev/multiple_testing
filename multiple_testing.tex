% arara: xelatex
\documentclass[12pt]{article}

% \usepackage{physics}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\usepackage{verse}

\usepackage{tikzducks}

\usepackage{tikz} % картинки в tikz
\usepackage{tkz-euclide}
\usetikzlibrary{shapes, arrows, positioning}
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering}

\usepackage{amsmath, amsfonts, amssymb} % куча стандартных математических плюшек

\usepackage{comment}

\usepackage[top=2cm, left=1.2cm, right=1.2cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption}

\usepackage{url} % to use \url{link to web}


\newcommand{\smallduck}{\begin{tikzpicture}[scale=0.3]
    \duck[
        cape=black,
        hat=black,
        mask=black
    ]
    \end{tikzpicture}}

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{}
\chead{Множественное тестирование}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\usepackage{tcolorbox} % рамочки!

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos - печатает все поставленные \todo'шки


% более красивые таблицы
\usepackage{booktabs}
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"


\setcounter{MaxMatrixCols}{20}
% by crazy default pmatrix supports only 10 cols :)


\usepackage{fontspec}
\usepackage{libertine}
\usepackage{polyglossia}

\setmainlanguage{english}
\setotherlanguages{russian}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
% \setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
% \newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
% \setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*}

%% эконометрические сокращения
\DeclareMathOperator{\Cov}{\mathbb{C}ov}
\DeclareMathOperator{\Corr}{\mathbb{C}orr}
\DeclareMathOperator{\Var}{\mathbb{V}ar}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\row}{row}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}

\DeclareMathOperator{\E}{\mathbb{E}}
% \DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\mgf}{mgf}

\DeclareMathOperator{\Convex}{Convex}
\DeclareMathOperator{\plim}{plim}

\usepackage{mathtools}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\scalp}{\langle}{\rangle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand{\cN}{\mathcal{N}}
\newcommand{\cF}{\mathcal{F}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\hb}{\hat{\beta}}
\newcommand{\dPois}{\mathrm{Pois}}


% arc, https://tex.stackexchange.com/questions/96680/
\makeatletter
\DeclareFontFamily{U}{tipa}{}
\DeclareFontShape{U}{tipa}{m}{n}{<->tipa10}{}
\newcommand{\arc@char}{{\usefont{U}{tipa}{m}{n}\symbol{62}}}%

\newcommand{\arc}[1]{\mathpalette\arc@arc{#1}}

\newcommand{\arc@arc}[2]{%
  \sbox0{$\m@th#1#2$}%
  \vbox{
    \hbox{\resizebox{\wd0}{\height}{\arc@char}}
    \nointerlineskip
    \box0
  }%
}
\makeatother




% dot of variable size, from
% https://tex.stackexchange.com/questions/389238/is-there-a-black-dot-symbol-that-i-can-use
\newcommand\vardot[1][.4]{\mathbin{\vcenter{\hbox{\scalebox{#1}{$\bullet$}}}}}
% dot above equality sign for Newton style
\newcommand{\ulteq}{\mathrel{\overset{\vardot}{=}}}
\newcommand{\ultsim}{\mathrel{\overset{\vardot}{\sim}}}
%\newcommand{\ulteq}{\mathrel{\overset{\infty}{=}}}
%\newcommand{\ultsim}{\mathrel{\overset{\infty}{\sim}}}


\newcommand{\angl}[1]{\widehat{#1}}
% \newcommand{\angl}[1]{\angle{#1}}

\begin{document}


\begin{verse}
    \begin{flushright}
fff
    \end{flushright}
\end{verse}


\begin{tcolorbox}[colback=yellow!50!red!25!white]
smth
\end{tcolorbox}


У нас есть $M$ нулевых гипотез, $H_{0, 1}$, $H_{0, 2}$, \dots, $H_{0, M}$, и $M$ соответствующих альтернативных гипотез,
$H_{A, 1}$, $H_{A, 2}$, \dots, $H_{A, M}$. 
Про каждую нулевую гипотезу мы должны принять решение, отвергнуть её или не отвергнуть. 
Для краткости, в обозначении нулевых гипотез будем пропускать ноль в индексе, $H_1$, $H_2$, \dots, $H_M$.
Скажем, у доктора есть $M$ пациентов, каждый из которых может быть здоров, $H_i$, или болен, $H_{A,i}$.

Для каждой гипотезы $H_i$ мы рассчитали её $p$-значение $p_i$. 
Алгоритмы, которые мы будем обсуждать, отвергают гипотезы с малыми $p_i$ и не отвергают гипотезы с большими $p_i$.
Вопрос лишь в том, что алгоритмы считают большим или малым $p$-значением. 
Для удобства обозначим упорядоченные по возрастанию $p$-значения как $p_{(1)} \leq p_{(2)} \leq \dots \leq p_{(M)}$.

Составим таблицу сопряжённости для всех возможных случаев. 
В клетках таблицы находятся количества соответствующих случаев:

\begin{tabular}{cccc}
    \toprule
    & $H_i$ верна & $H_i$ не верна & $\sum$ \\
    $H_i$ отвергнута & $FD$ & $TD$  & $D$ \\
    $H_i$ не отвергнута & $TN$ & $FN$  & $N$ \\
    \midrule
    $\sum$ & $M_0$ & $M_1$ & $M$ \\
    \bottomrule
\end{tabular}

Этимология сокращений проста. 
«Discoveries» $D$ — это количество обнаружений чего-то необычного, количество отвергнутых нулевых гипотез $H_i$.
Отвергнутые гипотезы делятся на верные обнаружения «true discoveries» $TD$ и ложные обнаружения «false discoveries» $FD$, $D = TD + FD$.
Аналогично, «non-discoveries» $N$ — это количество неотвергнутых нулевых гипотез. 
Неотвергнутые гипотезы делятся на верно неотвергнутые «true non-discoveries» $TN$ и ошибочно неотвергнутые «false non-disvoveries» $FN$, $N = TN + FN$.


Нас будут интересовать два показателя.
Во-первых, групповая вероятность ошибки первого рода, $FWER$, family wise error rate, вероятность хотя бы одной ошибочно отвергнутой гипотезы,
\[
FWER = \P(FD > 0).
\]
Во-вторых, средняя доля ошибочно отвергнутых гипотез среди всех отвергнутых, $FDR$, false discovery rate, 
\[
FDR = \E\left(\frac{FD}{D}\right).
\]
В ситуации, когда все нулевые гипотезы не отвергнуты, ошибочно отвергнутых гипотез тоже нет, $D = FD = 0$.
В этом случае формально дробь $FD/D$ не определена, и мы её доопределим нулём.
Более аккуратно можно определить,
\[
FDR = \E\left(\frac{FD}{\max\{1, D\}}\right).
\]


Групповую вероятность ошибки первого рода можно трактовать как ожидание индикатора,
$FWER = \E(I[FD > 0])$.
Индикатор $I[FD > 0]$ не бывает меньше доли $\frac{FD}{D}$, поэтому $FWER \geq FDR$.

Если мы знаем, что все нулевые гипотезы верны, $M_1 = 0$, то верных отвержение быть не может, $TD = 0$.
Следовательно, $FD = D$, и $FWER = FDR$.




Сначала рассмотрим несколько алгоритмов, которые гарантируют $FWER \leq \alpha$ при некоторых предпосылках.



Алгоритм Холма~— Бонферонни:

Определим гиперболу $q(i) = \alpha / (M + 1 - i)$. 
Отметим, что $q(1) = \alpha/M$, а $q(M) = \alpha$. 
Изначально считаем, что ни одна из гипотез $H_1$, \dots, $H_M$ не отвергается.
\begin{enumerate}
    \item Если $p_{(1)} \leq q(1) = \alpha / M$, то отвергаем $H_{(1)}$ и переходим к шагу 2. 
    Иначе выходим из алгоритма.
    \item Если $p_{(2)} \leq q(2) = 2\alpha / M$, то отвергаем $H_{(2)}$ и переходим к шагу 3. 
    Иначе выходим из алгоритма.

\end{enumerate}

TODO: здесь картинка

В результате применения алгоритма Холма~— Бонферонни $D$ гипотез с наименьшими $p_i$ оказываются отвергнуты. 
Первые $D$ штук $p$-значений $p_{(1)}$, $p_{(2)}$, \dots, $p_{(D)}$ лежат ниже или на гиперболе $q(u)$, 
а $p$-значение $p_{(D+1)}$ впервые оказывается выше гиперболы $q(u)$.


\begin{proof}
Для начала поинтересуемся верной отвергнутой гипотезой с наименьшим $p$-значением. 
Это гипотеза $H_{(N)}$ с $p$-значением $p_{(N)}$. 
По определению алгоритма, $p_{(N)}$ лежит не выше гиперболы, 
\[
p_{(N)} \leq \frac{\alpha}{M + 1 - N}.
\]
Левее это гипотезы лежат ложные гипотезы $H_{(1)}$, $H_{(2)}$, \dots, $H_{(N-1)}$.
Ложных гипотез всего $M - M_0$, поэтому 
\[
M - M_0 \geq N - 1
\]
Отсюда $M + 1 - N \geq M_0$ и 
\[
p_{(N)} \leq \frac{\alpha}{M + 1 - N} \leq \frac{\alpha}{M_0}.
\]
Другими словами, среди ошибочно отвергнутых гипотез есть гипотеза с $p$-значением меньше $\alpha/ M_0$.

Завершаем доказательство, 
\[
FWER = \P(FD > 0) \leq \P(\cup_{i \in I_0} p_i \leq \alpha / M_0) \leq \sum_{i \in I_0} \P(p_i \leq \alpha /M_0) = M_0 \cdot \frac{\alpha}{M_0} = \alpha.
\]
    
\end{proof}



\begin{proof}
    почти-доказательство:

    TODO: аккуратнее со случайностью подмножеств!

    Сначала рассмотрим подмножество гипотез, включающее гипотезу $H_{(1)}$. 
    В этом подмножестве не больше $M$ гипотез, проверка каждой (детали!) идёт на уровне значимости $\alpha / M$.
    Согласно методу Бонферонни, итоговая вероятность ошибочно отбросить хотя бы одну гипотезу из подмножества не превосходит $\alpha$.
    Теперь рассмотрим подмножество гипотез, включающее гипотезу $H_{(2)}$ и не включающее $H_{(1)}$. 
    В этом подмножестве не больше $M - 1$ гипотез, проверка каждой (детали!) идёт на уровне значимости $\alpha / (M - 1)$.
    Согласно методу Бонферонни, итоговая вероятность ошибочно отбросить хотя бы одну гипотезу из подмножества не превосходит $\alpha$.
    Далее аналогично рассматриваются все подмножества.     
\end{proof}





Теперь рассмотрим один алгоритм, который гарантирует $FDR \leq \alpha$.

Алгоритм Беньямини~— Хохберга:

Определим линейную функцию $q(i) = i \cdot \alpha / M$.
Отметим, что $q(1) = \alpha/M$, а $q(M) = \alpha$. 
Изначально считаем, что ни одна из гипотез $H_1$, \dots, $H_M$ не отвергается.
\begin{enumerate}
    \item Если $p_{(1)} \leq q(1) = \alpha / M$, то отвергаем $H_{(1)}$ и переходим к шагу 2. 
    Иначе выходим из алгоритма.
    \item Если $p_{(2)} \leq q(2) = 2\alpha / M$, то отвергаем $H_{(2)}$ и переходим к шагу 3. 
    Иначе выходим из алгоритма.

\end{enumerate}

TODO: здесь картинка

В результате применения алгоритма Беньямини~— Хохберга $D$ гипотез с наименьшими $p_i$ оказываются отвергнуты. 
Первые $D$ штук $p$-значений $p_{(1)}$, $p_{(2)}$, \dots, $p_{(D)}$ лежат ниже или на прямой $q(u)$, 
а $p$-значение $p_{(D+1)}$ впервые оказывается выше прямой $q(u)$.


\begin{proof}
Помимо количества отвергнутых алгоритмом гипотез $D$ введём дополнительную величину $D_i$.

TODO: проверить!

Представим себе, что мы принудительно отвергли гипотезу $i$ после применения алгоритма. 
Общее количество отвергнутых гипотез при этом окажется равно новой величине $D_i$. 
Конечно, величина $D_i$ либо равна $D$, либо на единицу больше. 

Величина $D_i$ нужна нам для тонкого разложения на сомножители. 
Заметим, что даже при независимых $p$-значениях $p_i$ величины $p_i$ и $D$ зависимы. 
Например, если $D = M$, то значит все гипотезы были отвергнуты и, согласно алгоритму, все $p_i$ были меньше некоторого своего порога. 
Однако, при независимых $p_i$ величины $p_i$ и $D_i$ независимы. 
Отсюда, 
\[
\P(p_i \leq q(d), D = d) = \P(p_i \leq q(d), D_i = d) = \P(p_i \leq q(d)) \P(D_i = d).
\]

Разложим общее число ошибочно отвергнутых гипотез в сумму индикаторов,
\[
FDR = \E\left( \frac{FD}{\max\{1, D\} }\right) = \sum_{i \in I_0} \E(I(p_i \leq q(D)) / D).
\]
В силу $\sum_d I[D = d] = 1$ получим дальнейше разрезание формулы:
\[
    FDR = \sum_{i \in I_0} \sum_{d=1}^M \E(I(D = d) I(p_i \leq q(D)) / D) = \sum_{i \in I_0}\sum_{d=1}^M \P(p_1 \leq q(d), D= d) / d.
\]
Применяем трюк с подменой $D$ на $D_i$ и вспоминаем о равномерном распределении $p_i$ при $i \in I_0$:
\[
    \P(p_i \leq q(d), D = d) = \P(p_i \leq q(d), D_i = d) = \P(p_i \leq q(d)) \P(D_i = d) = \frac{d \alpha}{M} \P(D_i = d).
\]
Тождество $\sum_d \P(D_i = d) = 1$ позволяет нам завершить вычисление $FDR$, 
\[
FDR = \sum_{i \in I_0} \sum_{d=1}^M \frac{\alpha}{M} \P(D_i = d) = \sum_{i \in I_0} \frac{\alpha}{M} = \frac{\alpha M_0}{M} \leq \alpha.
\]
\end{proof}



Разное:

$FWER \leq \alpha$ если и только если для любого подмножества $J \subset I_0$ вероятность отвергнуть хотя бы одну 
гипотезу из подмножества $J$ не превосходит $\alpha$. 

Доказательство. Если из множества $I_0$ удалить часть гипотез до получения множества $J$, то вероятность отвержения хотя бы одной из них не увеличивается.


\begin{proof}
Интуитивное почти-доказательство. Можно ли спасти?

Навесим условие внутри ожидания:
\[
\E\left( \frac{FD}{\max\{1, D\}} \right) = \E\left(\E\left( \frac{FD}{\max\{1, D\}} \mid D \right) \right)
\]
Рассмотрим условное ожидание:
\[
    \E\left( \frac{FD}{\max\{1, D\}} \mid D \right) = \E\left( \frac{FD}{\max\{1, D\}} \mid h(D) \right)
\]
Вот здесь опасный переход:
\[
    \E\left( \frac{FD}{\max\{1, D\}} \mid h(D) \right) = \frac{M_0 h(D)}{D} = \frac{M_0 \alpha D}{MD} = \frac{M_0 \alpha}{M}
\]
\end{proof}



\section*{Источники мудрости}



\end{document}

